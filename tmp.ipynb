{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0f722fbb15eed1cc4e3031568bb516693abeea6339598586c1c05bc1b31d796ed",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1.0, 0.0, 0.0, 0.0]\n[0.5, 0.5, 0.0, 0.0]\n[0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.0]\n[0.25, 0.25, 0.25, 0.25]\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 5):\n",
    "    weights = [1. / i] * i + [0.] * (4 - i)\n",
    "    print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"Now let’s try training a Transformer (Vaswani, et al 2017) ranker model. Make sure to complete this section on a GPU with PyTorch installed. We’ll be training on the Twitter task, which is a dataset of tweets and replies. There’s more information on tasks in these docs, including a full list of tasks Make sure Make sure and instructions on specifying arguments for training and evaluation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.07692307692307693\n"
     ]
    }
   ],
   "source": [
    "from parlai.core.metrics import IntraDistinctMetric\n",
    "print(float(IntraDistinctMetric.compute(s,ngram=2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "from parlai.core.metrics import InterDistinctMetric\n",
    "inter = InterDistinctMetric.compute(text=s, ngram=2).value()\n",
    "print(inter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.2143\n"
     ]
    }
   ],
   "source": [
    "from parlai.core.metrics import F1Metric\n",
    "y = \"Blue is my favorite primary color.\"\n",
    "knowledges = [\n",
    "                \" Blue is one of the three primary colours of pigments in painting and traditional colour theory, as well as in the RGB colour model.\",\n",
    "                \" It lies between violet and green on the spectrum of visible light.\",\n",
    "                \" The eye perceives blue when observing light with a dominant wavelength between approximately 450 and 495 nanometres.\",\n",
    "                \" Most blues contain a slight mixture of other colors; azure contains some green, while ultramarine contains some violet.\",\n",
    "                \" The clear daytime sky and the deep sea appear blue because of an optical effect known as Rayleigh scattering.\",\n",
    "                \" An optical effect called Tyndall scattering explains blue eyes.\",\n",
    "                \" Distant objects appear more blue because of another optical effect called atmospheric perspective.\",\n",
    "                \" Blue has been an important colour in art and decoration since ancient times.\",\n",
    "                \" The semi-precious stone lapis lazuli was used in ancient Egypt for jewellery and ornament and later, in The Renaissance, to make the pigment ultramarine, the most expensive of all pigments.\",\n",
    "                \" In the eighth century Chinese artists used cobalt blue to colour fine blue and white porcelain.\",\n",
    "                \" In the Middle Ages, European artists used it in the windows of Cathedrals.\",\n",
    "                \" Europeans wore clothing coloured with the vegetable dye woad until it was replaced by the finer indigo from America.\"\n",
    "            ]\n",
    "score = F1Metric.compute(y, knowledges)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/home/cbyou/Posterior-Knowledge-Selection\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'float'>\n56.54\n<class 'float'>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "loss = 56.54\n",
    "print(type(loss))\n",
    "loss_file_name = './snapshots/loss.json'\n",
    "if not os.path.exists(loss_file_name):\n",
    "    with open(loss_file_name, 'w', encoding='utf-8') as f:\n",
    "        d = {'loss':loss}\n",
    "        json.dump(d, f)\n",
    "else:\n",
    "    with open(loss_file_name, 'r', encoding='utf-8') as f:\n",
    "        d = json.load(f)\n",
    "        old_loss = d['loss']\n",
    "        print(old_loss)\n",
    "        print(type(old_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[-3.4402, -2.4402, -1.4402, -0.4402]], dtype=torch.float64)\ntensor([[ 1.0000e+00, 8.1363e-192, 2.6598e-185, 7.1246e-218]],\n       dtype=torch.float64)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(3.4402, dtype=torch.float64)"
      ]
     },
     "metadata": {},
     "execution_count": 132
    }
   ],
   "source": [
    "import torch\n",
    "a=torch.tensor([[1,2,3,4]],dtype=float)\n",
    "# b=a.clone()\n",
    "b=torch.tensor([[500,60,75,0]],dtype=float)\n",
    "# b[:,0]=1\n",
    "# b[:,1]=2\n",
    "\n",
    "a = torch.nn.functional.log_softmax(a,dim=1)\n",
    "b = torch.nn.functional.softmax(b,dim=1)\n",
    "print(a)\n",
    "print(b)\n",
    "my_loss = torch.nn.KLDivLoss(reduction='batchmean')\n",
    "my_loss(a,b)"
   ]
  }
 ]
}